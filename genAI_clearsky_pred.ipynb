{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5e1067-ac34-48b8-99d9-a78b35491bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "the next 60 days  value generated [6.9796743 6.9877954 6.9125123 6.7945027 6.706309  6.737358  6.820261\n",
      " 6.9158163 6.9968514 7.0227985 7.01037   6.991084  6.961612  6.9035363\n",
      " 6.8817782 6.858996  6.8673267 6.901821  6.951216  6.994352  7.0144296\n",
      " 6.944822  6.8360333 6.793413  6.7353506 6.704031  6.6767945 6.6896567\n",
      " 6.7685657 6.867928  6.9687843 7.0679884 7.1596065 7.209269  7.2481437\n",
      " 7.2679663 7.2869883 7.293232  7.3172917 7.2864046 7.2367373 7.159658\n",
      " 7.036342  6.95189   6.906763  6.8968287 6.895984  6.8872733 6.8909707\n",
      " 6.9007726 6.939664  6.9581275 6.9780755 6.991504  6.992429  6.9438076\n",
      " 6.9086814 6.895879  6.8876534 6.919152 ]\n"
     ]
    }
   ],
   "source": [
    "# Example of predicting on new data\n",
    "new_data = pd.read_csv('clearskypred.csv')  # Replace 'new1data.csv' with your new data file\n",
    "new_features = new_data[['clearsky']]  # Example: selecting relevant features\n",
    "\n",
    "# Normalize the new data using the same scaler\n",
    "scaled_new_features = scaler.transform(new_features)\n",
    "\n",
    "# Create sequences for the new data\n",
    "def create_sequences(data, seq_length):\n",
    "    X = []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        sequence = data[i:i + seq_length]\n",
    "        X.append(sequence)\n",
    "    return np.array(X)\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 10  # Adjust this as per your needs\n",
    "X_new = create_sequences(scaled_new_features, seq_length)\n",
    "\n",
    "# Make predictions on new data\n",
    "new_predictions = model.predict(X_new)\n",
    "\n",
    "# Inverse transform predictions to original scale\n",
    "new_predictions = scaler.inverse_transform(new_predictions)\n",
    "new_predictions = new_predictions.squeeze()\n",
    "# Print or use new_predictions as needed\n",
    "print(f'the next 60 days  value generated {new_predictions[:60]}')\n",
    "new_predictions = np.array(new_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99718cd3-598e-445c-be63-82f9fc40d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m10,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,651</span> (119.73 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m30,651\u001b[0m (119.73 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,651</span> (119.73 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m30,651\u001b[0m (119.73 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0513 - val_loss: 0.0064\n",
      "Epoch 2/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0058 - val_loss: 0.0060\n",
      "Epoch 3/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 4/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 5/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 6/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 7/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 8/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0045 - val_loss: 0.0038\n",
      "Epoch 10/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0044 - val_loss: 0.0037\n",
      "Epoch 11/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0044 - val_loss: 0.0038\n",
      "Epoch 12/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 13/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 14/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0037\n",
      "Epoch 15/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0045 - val_loss: 0.0038\n",
      "Epoch 16/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 17/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 18/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 19/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0043 - val_loss: 0.0037\n",
      "Epoch 20/20\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0037\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "Mean Squared Error: 35.5750334736073\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('bangaluru data.csv')  # Replace with your dataset\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = data[['clearsky']]  # Example features\n",
    "target = data['temp']  # Example target\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Create sequences for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        targets.append(data[i + seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "seq_length = 10\n",
    "X, y = create_sequences(scaled_features, seq_length)\n",
    "\n",
    "# Split into training and testing sets\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(seq_length, X.shape[2])))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_split=0.2, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "predictions = predictions.reshape(-1, 1)  # Reshape to (2520, 1) if predictions is (2520,)\n",
    "\n",
    "# Inverse transform predictions to original scale\n",
    "predictions = scaler.inverse_transform(predictions) \n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0c3c762-bc1c-4d2f-9b08-b1c673c9a436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "the next 60 days  value generated [7.1953197 7.071192  6.7272873 6.5571413 6.6258097 7.030036  7.186567\n",
      " 7.200803  7.1740084 7.0381765 6.9662356 6.965393  6.918672  6.807345\n",
      " 6.9489617 6.944676  7.0216517 7.0912876 7.1363473 7.115269  7.059113\n",
      " 6.7496767 6.602394  6.823009  6.7314305 6.764536  6.733331  6.850695\n",
      " 7.094686  7.1672783 7.201163  7.246761  7.3205476 7.2844934 7.316297\n",
      " 7.319289  7.3635387 7.35688   7.459536  7.2459087 7.174908  7.054977\n",
      " 6.8349915 6.9064636 6.9838095 7.0407143 7.016529  6.9456086 6.985174\n",
      " 6.9816484 7.101044  7.0260987 7.04435   7.065727  7.04224   6.8588247\n",
      " 6.9016495 6.969086  6.9666605 7.102627 ]\n"
     ]
    }
   ],
   "source": [
    "# Example of predicting on new data\n",
    "new_data = pd.read_csv('clearskypred.csv')  # Replace 'new1data.csv' with your new data file\n",
    "new_features = new_data[['clearsky']]  # Example: selecting relevant features\n",
    "\n",
    "# Normalize the new data using the same scaler\n",
    "scaled_new_features = scaler.transform(new_features)\n",
    "\n",
    "# Create sequences for the new data\n",
    "def create_sequences(data, seq_length):\n",
    "    X = []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        sequence = data[i:i + seq_length]\n",
    "        X.append(sequence)\n",
    "    return np.array(X)\n",
    "\n",
    "# Example usage:\n",
    "seq_length = 10  # Adjust this as per your needs\n",
    "X_new = create_sequences(scaled_new_features, seq_length)\n",
    "\n",
    "# Make predictions on new data\n",
    "new_predictions = model.predict(X_new)\n",
    "\n",
    "# Inverse transform predictions to original scale\n",
    "new_predictions = scaler.inverse_transform(new_predictions)\n",
    "new_predictions = new_predictions.squeeze()\n",
    "# Print or use new_predictions as needed\n",
    "print(f'the next 60 days  value generated {new_predictions[:60]}')\n",
    "new_predictions = np.array(new_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ae2e5a1-69ec-4132-afeb-264182775b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0541 - val_loss: 0.0066\n",
      "Epoch 2/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0060 - val_loss: 0.0062\n",
      "Epoch 3/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 4/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0056 - val_loss: 0.0058\n",
      "Epoch 5/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0055 - val_loss: 0.0046\n",
      "Epoch 6/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0044 - val_loss: 0.0041\n",
      "Epoch 7/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 8/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0044 - val_loss: 0.0038\n",
      "Epoch 10/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0045 - val_loss: 0.0038\n",
      "Epoch 11/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 12/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0044 - val_loss: 0.0041\n",
      "Epoch 13/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 14/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 15/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 16/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0044 - val_loss: 0.0038\n",
      "Epoch 17/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 18/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0047 - val_loss: 0.0040\n",
      "Epoch 19/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0039\n",
      "Epoch 20/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0038\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "The next 60 days values predicted: [7.2197995 7.07306   6.7035365 6.5446143 6.647585  7.0850954 7.209016\n",
      " 7.2086973 7.180067  7.0386176 6.974774  6.982434  6.928857  6.8073325\n",
      " 6.971853  6.9562926 7.038816  7.1080613 7.1512594 7.125193  7.066676\n",
      " 6.7313185 6.5949097 6.8614764 6.737026  6.7770677 6.7400484 6.8729844\n",
      " 7.128159  7.1845317 7.215424  7.261671  7.3365455 7.293728  7.3288884\n",
      " 7.3291306 7.373641  7.3628383 7.4701266 7.2407007 7.177468  7.0560184\n",
      " 6.825933  6.9246616 7.0034056 7.0557475 7.0242805 6.9496655 7.001409\n",
      " 6.9949446 7.1248503 7.0329065 7.056872  7.079382  7.052366  6.8539867\n",
      " 6.9179726 6.988488  6.9786105 7.1244345]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class LSTMModelPredictor:\n",
    "    def __init__(self, train_data_path, new_data_path):\n",
    "        self.train_data_path = train_data_path\n",
    "        self.new_data_path = new_data_path\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.is_scaler_fitted = False\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "        return data\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        features = data[['clearsky']]\n",
    "        target = data['temp']\n",
    "        scaled_features = self.scaler.fit_transform(features)\n",
    "        return scaled_features, target\n",
    "\n",
    "    def create_sequences(self, data, seq_length):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            sequences.append(data[i:i + seq_length])\n",
    "            targets.append(data[i + seq_length])\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    def build_model(self, seq_length):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(50, return_sequences=True, input_shape=(seq_length, 1)))\n",
    "        self.model.add(LSTM(50))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2):\n",
    "        history = self.model.fit(X_train, y_train, epochs=epochs, validation_split=validation_split, batch_size=batch_size)\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, y_true, y_pred):\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        return mse\n",
    "\n",
    "    def predict_new_data(self, new_data, seq_length):\n",
    "        scaled_new_features = self.scaler.transform(new_data)\n",
    "        X_new = self.create_sequences(scaled_new_features, seq_length)\n",
    "        new_predictions = self.model.predict(X_new)\n",
    "        new_predictions = self.scaler.inverse_transform(new_predictions)\n",
    "        new_predictions = new_predictions.squeeze()\n",
    "        return new_predictions\n",
    "\n",
    "    def run(self, seq_length=10, epochs=20, batch_size=32, validation_split=0.2):\n",
    "      \n",
    "          # Load training data\n",
    "        train_data = self.load_data(self.train_data_path)\n",
    "        # Preprocess training data\n",
    "        scaled_features, target = self.preprocess_data(train_data)\n",
    "        # Create sequences for LSTM\n",
    "        X_train, y_train = self.create_sequences(scaled_features, seq_length)\n",
    "        # Build LSTM model\n",
    "        self.build_model(seq_length)\n",
    "        # Train LSTM model\n",
    "        self.train_model(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "        # Example of predicting on new data\n",
    "        new_data = self.load_data(self.new_data_path)\n",
    "        new_features = new_data[['clearsky']]\n",
    "        # Predict on new data\n",
    "        new_predictions = self.predict_new_data(new_features, seq_length)\n",
    "        print(f'The next 60 days values predicted: {new_predictions[:60]}')\n",
    "        # Return or do further processing with new_predictions if needed\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to your datasets\n",
    "    train_data_path = 'bangaluru data.csv'\n",
    "    new_data_path = 'clearskypred.csv'\n",
    "    \n",
    "    # Initialize and run the LSTMModelPredictor\n",
    "    lstm_predictor = LSTMModelPredictor(train_data_path, new_data_path)\n",
    "    lstm_predictor.run(seq_length=10, epochs=20, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1695b60-59f6-46d1-b6f8-ad9f1b1cc206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
