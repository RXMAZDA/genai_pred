{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae2e5a1-69ec-4132-afeb-264182775b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0384 - val_loss: 0.0148\n",
      "Epoch 2/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0127 - val_loss: 0.0155\n",
      "Epoch 3/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 4/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0097 - val_loss: 0.0118\n",
      "Epoch 5/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 6/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 7/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0093 - val_loss: 0.0095\n",
      "Epoch 8/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0090 - val_loss: 0.0096\n",
      "Epoch 9/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 10/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 11/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 12/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 13/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 14/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 15/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0093 - val_loss: 0.0095\n",
      "Epoch 16/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0091 - val_loss: 0.0095\n",
      "Epoch 17/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0089 - val_loss: 0.0093\n",
      "Epoch 18/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0089 - val_loss: 0.0093\n",
      "Epoch 19/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0093 - val_loss: 0.0094\n",
      "Epoch 20/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0094 - val_loss: 0.0094\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "The next 60 days values predicted: [169.9732  148.55692 137.76778 240.83005 251.82135 257.77283 260.91455\n",
      " 284.05035 252.03307 171.70178 150.04013 190.4928  183.709   194.25362\n",
      " 260.72144 243.56923 262.55408 265.5079  285.13953 280.6179  204.3462\n",
      " 231.59335 232.63081 279.808   289.17505 296.04395 287.85574 270.6822\n",
      " 273.31952 273.63324 283.95642 278.70502 269.7727  268.80307 268.05228\n",
      " 269.2993  272.349   285.30148 277.2304  280.95767 281.25763 284.43887\n",
      " 270.71304 264.30142 294.2007  274.0437  265.10712 271.3094  274.2932\n",
      " 269.17053 275.59576 275.69647 270.23477 281.6923  277.99332 271.9354\n",
      " 259.54126 265.75034 267.66278 261.49136]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class LSTMModelPredictorwinddir:\n",
    "    def __init__(self, train_data_path, new_data_path):\n",
    "        self.train_data_path = train_data_path\n",
    "        self.new_data_path = new_data_path\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.is_scaler_fitted = False\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "        return data\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        features = data[['wind_direction']]\n",
    "        target = data['temp']\n",
    "        scaled_features = self.scaler.fit_transform(features)\n",
    "        return scaled_features, target\n",
    "\n",
    "    def create_sequences(self, data, seq_length):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            sequences.append(data[i:i + seq_length])\n",
    "            targets.append(data[i + seq_length])\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    def build_model(self, seq_length):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(50, return_sequences=True, input_shape=(seq_length, 1)))\n",
    "        self.model.add(LSTM(50))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2):\n",
    "        history = self.model.fit(X_train, y_train, epochs=epochs, validation_split=validation_split, batch_size=batch_size)\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, y_true, y_pred):\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        return mse\n",
    "\n",
    "    def predict_new_data(self, new_data, seq_length):\n",
    "        scaled_new_features = self.scaler.transform(new_data)\n",
    "        X_new = self.create_sequences(scaled_new_features, seq_length)\n",
    "        new_predictions = self.model.predict(X_new)\n",
    "        new_predictions = self.scaler.inverse_transform(new_predictions)\n",
    "        new_predictions = new_predictions.squeeze()\n",
    "        return new_predictions\n",
    "\n",
    "    def run(self, seq_length=10, epochs=20, batch_size=32, validation_split=0.2):\n",
    "      \n",
    "        # Load training data\n",
    "        train_data = self.load_data(self.train_data_path)\n",
    "        # Preprocess training data\n",
    "        scaled_features, target = self.preprocess_data(train_data)\n",
    "        # Create sequences for LSTM\n",
    "        X_train, y_train = self.create_sequences(scaled_features, seq_length)\n",
    "        # Build LSTM model\n",
    "        self.build_model(seq_length)\n",
    "        # Train LSTM model\n",
    "        self.train_model(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "        # Example of predicting on new data\n",
    "        new_data = self.load_data(self.new_data_path)\n",
    "        new_features = new_data[['wind_direction']]\n",
    "        # Predict on new data\n",
    "        new_predictions = self.predict_new_data(new_features, seq_length)\n",
    "        print(f'The next 60 days values predicted: {new_predictions[:60]}')\n",
    "        # Return or do further processing with new_predictions if needed\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to your datasets\n",
    "    train_data_path = 'bangaluru data.csv'\n",
    "    new_data_path = 'wind_dir_pred.csv'\n",
    "    \n",
    "    # Initialize and run the LSTMModelPredictor\n",
    "    lstm_predictor = LSTMModelPredictorwinddir(train_data_path, new_data_path)\n",
    "    lstm_predictor.run(seq_length=10, epochs=20, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1695b60-59f6-46d1-b6f8-ad9f1b1cc206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
