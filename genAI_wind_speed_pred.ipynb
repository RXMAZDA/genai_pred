{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae2e5a1-69ec-4132-afeb-264182775b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0224 - val_loss: 0.0106\n",
      "Epoch 2/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0089 - val_loss: 0.0067\n",
      "Epoch 3/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 4/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 5/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0057 - val_loss: 0.0066\n",
      "Epoch 6/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 7/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 8/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0057 - val_loss: 0.0062\n",
      "Epoch 9/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 10/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0058 - val_loss: 0.0061\n",
      "Epoch 11/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0058 - val_loss: 0.0060\n",
      "Epoch 12/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0060 - val_loss: 0.0061\n",
      "Epoch 13/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0059 - val_loss: 0.0061\n",
      "Epoch 14/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0059 - val_loss: 0.0067\n",
      "Epoch 15/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 16/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0057 - val_loss: 0.0060\n",
      "Epoch 17/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0058 - val_loss: 0.0066\n",
      "Epoch 18/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 19/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0057 - val_loss: 0.0060\n",
      "Epoch 20/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0057 - val_loss: 0.0060\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "The next 60 days values predicted: [2.5693302 3.1927626 2.862298  2.6627262 3.9955304 4.043826  3.6253498\n",
      " 3.296247  1.9884871 2.9437728 2.5732994 3.017372  2.3728    3.0590518\n",
      " 2.619884  3.181691  2.996115  3.299832  3.3450449 3.1258748 3.1819654\n",
      " 2.7470062 2.1998587 2.8703396 3.2164721 3.4130812 3.7545152 3.3175714\n",
      " 4.0774927 4.196619  3.7372892 4.569001  6.416102  5.874112  4.85048\n",
      " 4.6734185 6.571133  5.1069098 4.8983703 4.858954  4.5187654 4.356727\n",
      " 3.2486756 3.3431027 3.7169328 4.413562  4.7295356 4.135341  5.3598304\n",
      " 5.209977  5.586791  5.3117347 5.9438453 5.9374638 6.0967274 6.271072\n",
      " 6.2253985 6.919292  7.390474  6.2260265]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class LSTMModelPredictowind:\n",
    "    def __init__(self, train_data_path, new_data_path):\n",
    "        self.train_data_path = train_data_path\n",
    "        self.new_data_path = new_data_path\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.is_scaler_fitted = False\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "        return data\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        features = data[['wind']]\n",
    "        target = data['temp']\n",
    "        scaled_features = self.scaler.fit_transform(features)\n",
    "        return scaled_features, target\n",
    "\n",
    "    def create_sequences(self, data, seq_length):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            sequences.append(data[i:i + seq_length])\n",
    "            targets.append(data[i + seq_length])\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    def build_model(self, seq_length):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(50, return_sequences=True, input_shape=(seq_length, 1)))\n",
    "        self.model.add(LSTM(50))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2):\n",
    "        history = self.model.fit(X_train, y_train, epochs=epochs, validation_split=validation_split, batch_size=batch_size)\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, y_true, y_pred):\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        return mse\n",
    "\n",
    "    def predict_new_data(self, new_data, seq_length):\n",
    "        scaled_new_features = self.scaler.transform(new_data)\n",
    "        X_new = self.create_sequences(scaled_new_features, seq_length)\n",
    "        new_predictions = self.model.predict(X_new)\n",
    "        new_predictions = self.scaler.inverse_transform(new_predictions)\n",
    "        new_predictions = new_predictions.squeeze()\n",
    "        return new_predictions\n",
    "\n",
    "    def run(self, seq_length=10, epochs=20, batch_size=32, validation_split=0.2):\n",
    "      \n",
    "        # Load training data\n",
    "        train_data = self.load_data(self.train_data_path)\n",
    "        # Preprocess training data\n",
    "        scaled_features, target = self.preprocess_data(train_data)\n",
    "        # Create sequences for LSTM\n",
    "        X_train, y_train = self.create_sequences(scaled_features, seq_length)\n",
    "        # Build LSTM model\n",
    "        self.build_model(seq_length)\n",
    "        # Train LSTM model\n",
    "        self.train_model(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "        # Example of predicting on new data\n",
    "        new_data = self.load_data(self.new_data_path)\n",
    "        new_features = new_data[['wind']]\n",
    "        # Predict on new data\n",
    "        new_predictions = self.predict_new_data(new_features, seq_length)\n",
    "        print(f'The next 60 days values predicted: {new_predictions[:60]}')\n",
    "        # Return or do further processing with new_predictions if needed\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to your datasets\n",
    "    train_data_path = 'bangaluru data.csv'\n",
    "    new_data_path = 'wind_s_pred.csv'\n",
    "    \n",
    "    # Initialize and run the LSTMModelPredictor\n",
    "    lstm_predictor = LSTMModelPredictowind(train_data_path, new_data_path)\n",
    "    lstm_predictor.run(seq_length=10, epochs=20, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1695b60-59f6-46d1-b6f8-ad9f1b1cc206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
