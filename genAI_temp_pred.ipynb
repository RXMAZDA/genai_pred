{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae2e5a1-69ec-4132-afeb-264182775b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0184 - val_loss: 0.0037\n",
      "Epoch 2/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0033\n",
      "Epoch 3/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0029\n",
      "Epoch 4/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 5/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 6/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 7/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 8/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 9/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 10/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 11/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 12/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 13/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 14/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 15/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 16/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 17/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 18/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 19/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 20/20\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "The next 60 days values predicted: [26.73973  27.357296 27.97815  28.316177 27.774826 28.890013 28.671381\n",
      " 28.533968 29.37593  29.124554 27.953985 28.089518 28.539984 28.477491\n",
      " 27.974655 26.989613 27.31319  28.038418 27.701042 27.768694 28.326422\n",
      " 27.70086  27.067795 26.674063 26.87807  28.179    28.322584 26.730822\n",
      " 27.43221  27.431274 25.606094 26.965233 27.051395 27.062147 26.89262\n",
      " 27.131992 26.767876 27.696674 27.583235 27.076279 26.072014 26.113514\n",
      " 25.313883 24.620987 25.819765 25.404778 24.36221  24.826897 25.157354\n",
      " 25.008123 25.313068 25.196535 25.15817  25.503551 24.312626 24.634296\n",
      " 24.839474 23.855211 24.086111 24.887115]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class LSTMModelPredictortemp:\n",
    "    def __init__(self, train_data_path, new_data_path):\n",
    "        self.train_data_path = train_data_path\n",
    "        self.new_data_path = new_data_path\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.is_scaler_fitted = False\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "        return data\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        features = data[['temp']]\n",
    "        target = data['clearsky']\n",
    "        scaled_features = self.scaler.fit_transform(features)\n",
    "        return scaled_features, target\n",
    "\n",
    "    def create_sequences(self, data, seq_length):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            sequences.append(data[i:i + seq_length])\n",
    "            targets.append(data[i + seq_length])\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    def build_model(self, seq_length):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(50, return_sequences=True, input_shape=(seq_length, 1)))\n",
    "        self.model.add(LSTM(50))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2):\n",
    "        history = self.model.fit(X_train, y_train, epochs=epochs, validation_split=validation_split, batch_size=batch_size)\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, y_true, y_pred):\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        return mse\n",
    "\n",
    "    def predict_new_data(self, new_data, seq_length):\n",
    "        scaled_new_features = self.scaler.transform(new_data)\n",
    "        X_new = self.create_sequences(scaled_new_features, seq_length)\n",
    "        new_predictions = self.model.predict(X_new)\n",
    "        new_predictions = self.scaler.inverse_transform(new_predictions)\n",
    "        new_predictions = new_predictions.squeeze()\n",
    "        return new_predictions\n",
    "\n",
    "    def run(self, seq_length=10, epochs=20, batch_size=32, validation_split=0.2):\n",
    "      \n",
    "        # Load training data\n",
    "        train_data = self.load_data(self.train_data_path)\n",
    "        # Preprocess training data\n",
    "        scaled_features, target = self.preprocess_data(train_data)\n",
    "        # Create sequences for LSTM\n",
    "        X_train, y_train = self.create_sequences(scaled_features, seq_length)\n",
    "        # Build LSTM model\n",
    "        self.build_model(seq_length)\n",
    "        # Train LSTM model\n",
    "        self.train_model(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "        # Example of predicting on new data\n",
    "        new_data = self.load_data(self.new_data_path)\n",
    "        new_features = new_data[['temp']]\n",
    "        # Predict on new data\n",
    "        new_predictions = self.predict_new_data(new_features, seq_length)\n",
    "        print(f'The next 60 days values predicted: {new_predictions[:60]}')\n",
    "        # Return or do further processing with new_predictions if needed\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to your datasets\n",
    "    train_data_path = 'bangaluru data.csv'\n",
    "    new_data_path = 'temppred.csv'\n",
    "    \n",
    "    # Initialize and run the LSTMModelPredictor\n",
    "    lstm_predictor = LSTMModelPredictortemp(train_data_path, new_data_path)\n",
    "    lstm_predictor.run(seq_length=10, epochs=20, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1695b60-59f6-46d1-b6f8-ad9f1b1cc206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
